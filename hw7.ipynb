{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 7: Decision Trees\n",
    "\n",
    "_**NOTE**: this assignment is only mandatory for students registered for CSC 8515; students registered for CSC 4515 are **not** required to complete this assignment._\n",
    "\n",
    "For this assignment, your goal is to implement the ID3 algorithm (greedy information-gain based decision tree induction).  You should use the Congressional Voting Records data set, the same one that we used with Naive Bayes, and for the same reason; it is a discrete multivariate dataset, which is well suited to decision trees.  \n",
    "\n",
    "Note that while scikit-learn does have decision trees, it doesn't really handle discrete data well, so the library version may not produce exactly the same trees that your algorithm will.\n",
    "\n",
    "Implement and test your algorithm; be sure you can print the actual tree (a simple text-based visualization is fine), as well as calculating classification accuracy on held-out testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-39-a65aee6580c9>, line 131)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-a65aee6580c9>\"\u001b[0;36m, line \u001b[0;32m131\u001b[0m\n\u001b[0;31m    return classifyNoisy(matchingChildren[0], point)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self, parent=None):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.splitFeature = None\n",
    "        self.splitFeatureValue = None\n",
    "        self.label = None\n",
    "\n",
    "\n",
    "def dataToDistribution(data):\n",
    "    ''' Turn a dataset which has n possible classification labels into a\n",
    "        probability distribution with n entries. '''\n",
    "    allLabels = [label for (point, label) in data]\n",
    "    numEntries = len(allLabels)\n",
    "    possibleLabels = set(allLabels)\n",
    "\n",
    "    dist = []\n",
    "    for aLabel in possibleLabels:\n",
    "        dist.append(float(allLabels.count(aLabel)) / numEntries)\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def entropy(dist):\n",
    "    ''' Compute the Shannon entropy of the given probability distribution. '''\n",
    "    return -sum([p * math.log(p, 2) for p in dist])\n",
    "\n",
    "\n",
    "def splitData(data, featureIndex):\n",
    "    ''' Iterate over the subsets of data corresponding to each value\n",
    "        of the feature at the index featureIndex. '''\n",
    "\n",
    "    # get possible values of the given feature\n",
    "    attrValues = [point[featureIndex] for (point, label) in data]\n",
    "\n",
    "    for aValue in set(attrValues):\n",
    "        dataSubset = [(point, label) for (point, label) in data\n",
    "                      if point[featureIndex] == aValue]\n",
    "\n",
    "        yield dataSubset\n",
    "\n",
    "\n",
    "def gain(data, featureIndex):\n",
    "    ''' Compute the expected gain from splitting the data along all possible\n",
    "        values of feature. '''\n",
    "\n",
    "    entropyGain = entropy(dataToDistribution(data))\n",
    "\n",
    "    for dataSubset in splitData(data, featureIndex):\n",
    "        entropyGain -= entropy(dataToDistribution(dataSubset))\n",
    "\n",
    "    return entropyGain\n",
    "\n",
    "\n",
    "def homogeneous(data):\n",
    "    ''' Return True if the data have the same label, and False otherwise. '''\n",
    "    return len(set([label for (point, label) in data])) <= 1\n",
    "\n",
    "\n",
    "def majorityVote(data, node):\n",
    "    ''' Label node with the majority of the class labels in the given data set. '''\n",
    "    labels = [label for (pt, label) in data]\n",
    "    choice = max(set(labels), key=labels.count)\n",
    "    node.label = choice\n",
    "    return node\n",
    "\n",
    "\n",
    "def buildDecisionTree(data, root, remainingFeatures):\n",
    "    ''' Build a decision tree from the given data, appending the children\n",
    "        to the given root node (which may be the root of a subtree). '''\n",
    "\n",
    "    if homogeneous(data):\n",
    "        root.label = data[0][1]\n",
    "        return root\n",
    "\n",
    "    if len(remainingFeatures) == 0:\n",
    "        return majorityVote(data, root)\n",
    "\n",
    "    # find the index of the best feature to split on\n",
    "    bestFeature = max(remainingFeatures, key=lambda index: gain(data, index))\n",
    "\n",
    "    if gain(data, bestFeature) == 0:\n",
    "        return majorityVote(data, root)\n",
    "\n",
    "    root.splitFeature = bestFeature\n",
    "    for dataSubset in splitData(data, bestFeature):\n",
    "        aChild = Tree(parent=root)\n",
    "        aChild.splitFeatureValue = dataSubset[0][0][bestFeature]\n",
    "        root.children.append(aChild)\n",
    "\n",
    "        buildDecisionTree(dataSubset, aChild,\n",
    "                          remainingFeatures - set([bestFeature]))\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "def decisionTree(data):\n",
    "    return buildDecisionTree(data, Tree(), set(range(len(data[0][0]))))\n",
    "\n",
    "\n",
    "def classify(tree, point):\n",
    "    ''' Classify a data point by traversing the given decision tree. '''\n",
    "\n",
    "    if tree.children == []:\n",
    "        return tree.label\n",
    "    else:\n",
    "        matchingChildren = [child for child in tree.children\n",
    "                            if child.splitFeatureValue == point[tree.splitFeature]]\n",
    "\n",
    "    child = Tree()\n",
    "    if matchingChildren:\n",
    "        child = matchingChildren[0]\n",
    "    return classify(child, point)\n",
    "\n",
    "def classifyNoisy(tree, point):\n",
    "    ''' Classify a noisy data point by traversing the given decision tree.\n",
    "       Return a dictionary of the appropriate class counts to account for\n",
    "       multiple branching. '''\n",
    "\n",
    "    if tree.children == []:\n",
    "        return tree.classCounts\n",
    "    elif point[tree.splitFeature] == '?':\n",
    "        dicts = [classifyNoisy(child, point) for child in tree.children]\n",
    "        return dictionarySum(*dicts)\n",
    "    else:\n",
    "        matchingChildren = [child for child in tree.children\n",
    "        if child.splitFeatureValue == point[tree.splitFeature]]\n",
    "            return classifyNoisy(matchingChildren[0], point)\n",
    "\n",
    "\n",
    "def classify2(tree, point):\n",
    "   ''' Classify data which is assumed to have the possibility of being noisy.\n",
    "       Return the label corresponding to the maximum among all possible\n",
    "       continuations of the tree traversal. That is, the maximum of the\n",
    "       class counts at the leaves. Classify2 is equivalent to classify\n",
    "       if the data are not noisy. If the data are noisy, classify will\n",
    "       raise an error. '''\n",
    "   counts = classifyNoisy(tree, point)\n",
    "\n",
    "   if len(counts.keys()) == 1:\n",
    "      return counts.keys()[0]\n",
    "   else:\n",
    "      return max(counts.keys(), key=lambda k: counts[k])\n",
    "\n",
    "\n",
    "def testClassification(data, tree):\n",
    "    actualLabels = [label for point, label in data]\n",
    "    predictedLabels = [classify(tree, point) for point, label in data]\n",
    "\n",
    "    correctLabels = [(1 if a == b else 0) for a, b in zip(actualLabels, predictedLabels)]\n",
    "    return float(sum(correctLabels)) / len(actualLabels)\n",
    "\n",
    "def testTreeSize(noisyData, cleanData):\n",
    "    import random\n",
    "\n",
    "    for i in range(1, len(cleanData)):\n",
    "        tree = decisionTree(random.sample(cleanData, i))\n",
    "    print str(testClassification(noisyData, tree)) + \", \",\n",
    "\n",
    "with open('house-votes-84.data', 'r') as inputFile:\n",
    "    lines = inputFile.readlines()\n",
    "\n",
    "data = [line.strip().split(',') for line in lines]\n",
    "data = [(x[1:], x[0]) for x in data]\n",
    "\n",
    "cleanData = [x for x in data if '?' not in x[0]]\n",
    "noisyData = [x for x in data if '?' in x[0]]\n",
    "\n",
    "# tree = decisionTree(cleanData)\n",
    "tree = decisionTree(cleanData)\n",
    "testClassification(cleanData, tree)\n",
    "\n",
    "printBinaryDecisionTree(tree)\n",
    "print(classify(tree, ['y' for _ in range(len(data[0][0]))]))\n",
    "print(classify(tree, ['n' for _ in range(len(data[0][0]))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
